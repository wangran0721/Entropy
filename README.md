# Entropy
一个关于信息熵的小实验：
前几天读了吴老师的《数学之美》的第六章，关于【同样长度的书，所含信息量可以相差很多】这句话，做了一个小实验。
一：
同语义下中文词汇信息熵和英文词汇信息熵大小比较：
![‘做’所占的比特]
![‘make’所占的比特]
二：
用python脚本在一篇文档中写入500000个“啪”字和500000个随机字符
通过比特大小来查看信息量大小。
![500000个“啪”字所占的比特]
![500000个随机字符所占的比特](http://yun.baidu.com/disk/home?errno=0&errmsg=Auth%20Login%20Sucess&stoken=4864531f4020a78aaacca64ceb6c6b933307782c6d7658f365702486fd4969a036654819eae2f75d519c156c5f712178ec2ccc3ae0edde143c4d88535587ab5db0199fe093cf&bduss=a505a8b4c566877ba03cdfaf438b3150ecc05bba0fdf87b9834e0575764a845a36cb9807d4563d4134b9131196ab40c0c71f7471c0731577a5316d1a7492fb75014da1a94bc73d8a4bbd9dbf64e5e072f54c166a3f88a94336338d6a7f1cb3dfeb39f87f13eb898c8604a7a118296f615171bf8a57eca49948ae150eceff66a908a68f9a063b7f4be689810a4e8d924b2528046eef8d908c64c15b5c65583d6310788519cd107eb47ddcca9fb39f094573d29baaa5a2636a5be1b1a50fa89c6ee075585b760e&ssnerror=0#list/vmode=list&path=%2F1)
